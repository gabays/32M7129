---
title: "Cours_Geneve_13"
author: "Simon Gabay"
date: "`r Sys.Date()`"
#output:
#  html_notebook:
#    number_sections: yes
#    toc: yes
#    toc_float: yes
---

```{r, setup, fig.show=hold, fig.margin=TRUE}
if(!require("knitr")){
  install.packages("knitr")
  library(knitr)
}
knitr::opts_chunk$set(echo = TRUE, fig.width=20)
```

**
N'OUBLIEZ PAS DE RENSEIGNER LE NOM DE L'OEUVRE
**

## Préparer la session de travail

```{r}
setwd("~/GitHub/Cours_2020_UniGE/Cours_Geneve_13")
```

# 0. Préparation du texte

## Udpipe

Il va falloire télécharger le package `UDpipe`. Pour le français du XVIIème s. nous conseillons `french-gsd`, même s'il est (très) loin d'être parfait.

```{r, warning=FALSE, results='hide'}
if (!require("udpipe")) install.packages("udpipe")
library(udpipe)
```

## Nom du texte

Vous pouvez ici rentrer le nom du texte que nous allons étudier. Le nom que vous allez mettre va être reproduit sur tous les graphiques: attention!
Enter the name of the text that will be studied (it is the one that will be printed on all the graphs produced here), and the name of the file

```{r}
text_title <- "Boyer - Agamemnon (1680)"
file_name<- "BOYER_AGAMEMNON_1680/BOYER_AGAMEMNON_1680.txt"
#Il pesut être utile de créer un dossier spécifique, pour mettre toutes les images produites dans cet endroit
folder_name <-"BOYER_AGAMEMNON_1680/"
```

## Préparation de la lemmatisation

Nous allons lemamtiser le texte à l'aide dun modèle `UDPipe`. Il est en effet plus facile de travailler sur des lemmes pour diminuer la taille du lexique.

Si vous n'avez pas de modèles (il y en a un dans le dossier), vous pouvez les télécharger à cet endroit.

If you do not have an udpipe model, you can download one here (and adjust _infra_ the name)
```{r}
#m<-udpipe_download_model(language =  "french-gsd")
#m<-udpipe_download_model(language =  "french-partut")
#m<-udpipe_download_model(language =  "french-sequoia")
```

Nous pouvons désormais charger le texte, et le modèle.

```{r}
udpipe_model_french <- udpipe_load_model(file = "tools/french-gsd-ud-2.4-190531.udpipe")
text_to_lemmatise <- readLines(file_name)
#variable pour les sauvegardes
#nom de fichier sans extension
file_name_noExt <- tools::file_path_sans_ext(file_name)
#ajout d'une extention
filename <- paste(file_name_noExt,"_LEM.txt", sep="")
```

# Lemmatisation

```{r}
#on transforme le texte en une longue chaîne de caractères séparés par des espaces.
text_to_lemmatise <- paste(text_to_lemmatise, collapse = " ")
# on lemmatise cette chaîne
x <- udpipe_annotate(udpipe_model_french, x = text_to_lemmatise)
#je fais un data.frame
x <- as.data.frame(x)
# voir un extrait
str(x)
```

Je sauvegarde le texte
```{r}
cat(x$lemma[1], "", file = filename)
for(i in 2:length(x$token_id)){
  if(x$sentence_id[i] != x$sentence_id[i-1])
    cat("\n", file = filename, append = T)
  if(is.na(x$lemma[i]))
    next
  cat(x$lemma[i], "", file = filename, append = T)
}
```

# 1. Vers l'analyse de sentiment

## Téélcharger et charger le package syuzhet

```{r, warning=FALSE, results='hide'}
if (!require("syuzhet")) install.packages("syuzhet")
library(syuzhet)
```

## Charger un lexique

Amine Abdaoui, Jérôme Azé, Sandra Bringay et Pascal Poncelet. "FEEL: French Expanded Emotion Lexicon. Language Resources and Evaluation", _LRE 2016_, pp 1-23. Cf. http://www.lirmm.fr/~abdaoui/FEEL

Cf. aussi the package R `rfeel`: https://github.com/ColinFay/rfeel

```{r}
FEEL = read.csv("lexicons/FEEL-1_clean.csv")
load("lexicons/sentiments_polarity.rda")
load("lexicons/sentiments_score.rda")
```

On peut jeter un petit coup d'œil à ces lexiques

```{r}
View(FEEL)
View(sentiments_polarity)
View(sentiments_score)
```

## Importer le texte

Il faut désormais importer le texte que nous allons étudier

```{r, warning=FALSE, results='hide'}
text_to_analyse <- readLines(filename)
head(text_to_analyse)
```

# 2. Polarity scores

## Création du vecteur
Nous utilisons la méthode de Bing pour étudier la polarité: chaque token est comparé à un lexique, où des mots sont catégorés comme positifs ou négatifs.

```{r}
text_to_analyse_bing_vector <- get_sentiment(text_to_analyse, method = "bing", lexicon = FEEL)
head(text_to_analyse_bing_vector, 300)
#summary(text_to_analyse_bing_vector)
```

Nous pouvons donc désormais proposer une première visualisation

```{r, warning=FALSE, results='hide'}
simple_plot(text_to_analyse_bing_vector, title = paste(text_title, " - BING"))

# on sauvegarde l'image
png(paste(file_name_noExt,"_BING.png", sep=""), height = 900, width = 1600, res = 100)
simple_plot(text_to_analyse_bing_vector, title =  paste(text_title, " - BING"))
dev.off()
```

## Normalisation de la visualisation

Nous avons besoin de normaliser la longueur de notre image pour pouvoir comparer des textes de longueurs différentes. On peut aussi diminuer la taille  des chunks pour amplifier les tendances.

Avec une base 100

```{r, warning=FALSE, results='hide'}
percent_vals <- get_percentage_values(text_to_analyse_bing_vector, bins = 100)
plot(
  percent_vals, 
  type="l", 
  main=paste(text_title,"base 100", sep=" "), 
  xlab = "Narrative Time", 
  ylab= "Emotional Valence", 
  col="red"
  )
#Saving the graph
png(paste(file_name_noExt,"_BASE100.png"), height = 900, width = 1600, res = 100)
plot(
  percent_vals, 
  type="l", 
  main=paste(text_title,"base 100", sep=" "), 
  xlab = "Narrative Time", 
  ylab= "Emotional Valence", 
  col="red"
  )
dev.off()
```

Avec une base  10 

```{r, warning=FALSE, results='hide'}
percent_vals <- get_percentage_values(text_to_analyse_bing_vector, bins = 10)
plot(
  percent_vals, 
  type="l", 
  main=paste(text_title,"base 10", sep=" "), 
  xlab = "Narrative Time", 
  ylab= "Emotional Valence", 
  col="red"
  )
#Saving the graph
png(paste(file_name_noExt,"_BASE10.png"), height = 900, width = 1600, res = 100)
plot(
  percent_vals, 
  type="l", 
  main=paste(text_title,"base 100", sep=" "), 
  xlab = "Narrative Time", 
  ylab= "Emotional Valence", 
  col="red"
  )
dev.off()
```

Avec une base 2

```{r, warning=FALSE, results='hide'}
percent_vals <- get_percentage_values(text_to_analyse_bing_vector, bins = 2)
plot(
  percent_vals, 
  type="l", 
  main=paste(text_title,"base 2", sep=" "), 
  xlab = "Narrative Time", 
  ylab= "Emotional Valence", 
  col="red"
  )
#Saving the graph
png(paste(file_name_noExt,"_BASE2.png"), height = 900, width = 1600, res = 100)
plot(
  percent_vals, 
  type="l", 
  main=paste(text_title,"base 2", sep=" "), 
  xlab = "Narrative Time", 
  ylab= "Emotional Valence", 
  col="red"
  )
dev.off()
```

On peut obtenir une tendance générale avec `summary`

```{r}
sum(text_to_analyse_bing_vector)
summary(text_to_analyse_bing_vector)
```

# 3. Etudier les sentiments

## La joie

### Nous allons customier le dictionnaire

Pour étudier la joie, nous sortons la colonne joie du lexique pour créer un petit sous-lexique

```{r}
custom_dict_joy <- data.frame(word=FEEL$word, value=FEEL$joy)
head(custom_dict_joy)
```

Je reproduis là même manipulation que _supra_

```{r, warning=FALSE, results='hide'}
text_to_analyse_vector_joy <- get_sentiment(text_to_analyse, method = "custom", lexicon = custom_dict_joy)
simple_plot(text_to_analyse_vector_joy, title = paste(text_title, "Joy", sep=" - "))
#Saving the graph
png(paste(file_name_noExt,"_JOY.png"), height = 900, width = 1600, res = 100)
simple_plot(text_to_analyse_vector_joy, title = paste(text_title, "Joy", sep=" - "))
dev.off()
```

### Un peu d'ingénierie

Qu'est-ce qu'il y a sous le capot

![100% center](images/garage.jpg)

nous allons regarder les mots qui ont été utilisés  pendant l'analyse, classés par fréquence

```{r}
text_to_analyse_tokenised <- table(unlist(strsplit(text_to_analyse, "\\W")))
head(sort(text_to_analyse_tokenised, decreasing = T), n=50)
```

Je peux extraire tous les mots utiles pour l'analye

```{r}
positive_words=character()
positive_words_custom=character()
for(i in 1:length(text_to_analyse_tokenised)){
  # identify positive words
  if(get_sentiment(names(text_to_analyse_tokenised[i]), method = "custom", lexicon = custom_dict_joy) > 0){
    positive_words = c(positive_words, names(text_to_analyse_tokenised[i]))
    positive_words_custom = c(positive_words_custom, (text_to_analyse_tokenised[i]*get_sentiment(names(text_to_analyse_tokenised[i]), method = "custom", lexicon = custom_dict_joy)))
  }
}
head(sort(positive_words_custom, decreasing = T), n=50)
```

## Attention!!

Regardons les mots qui sont catégorisés comme de la joie d'après FEEL

```{r}
head(FEEL$word[which(FEEL$joy==1)], n=50)
```

Le lexique est (vraiment) loin d'êtr parfait: des mots comme _content_ ou _fier_ ne sont pas considéré comme relevant de la joie!

```{r}
FEEL$joy[which(FEEL$word=="joie")]
FEEL$joy[which(FEEL$word=="heureux")]
FEEL$joy[which(FEEL$word=="content")]
FEEL$joy[which(FEEL$word=="fier")]
```

Et donc la phrase "je suis heureux, content et fier" (ou plutôt `je être heureux, content et fier`) a un score de 1.

```{r}
#je rentre mon texte
text_to_analyse_test <-"je suis heureux, content et fier"
#Je tokenise
text_to_analyse_tokenised_test <- table(unlist(strsplit(text_to_analyse_test, "\\W")))
#Je remets mes variables à zero
positive_words=character()
positive_words_custom=character()
#Je cherche dans le lexique
#Pour chaque mot de ma phrase
for(i in 1:length(text_to_analyse_tokenised_test)){
  # si un mot a une valeur supérieure à zéro dans mon lexique
  if(get_sentiment(names(text_to_analyse_tokenised_test[i]), method = "custom", lexicon = custom_dict_joy) > 0){
    #Je l'ajoute aux mots positifs
    positive_words = c(positive_words, names(text_to_analyse_tokenised_test[i]))
    positive_words_custom = c(positive_words_custom, (text_to_analyse_tokenised_test[i]*get_sentiment(names(text_to_analyse_tokenised_test[i]), method = "custom", lexicon = custom_dict_joy)))
  }
}
positive_words_custom
```

## Peur

Je peux recommencer la même étude, mais cette fois avec le lexique de la  peur

```{r}
custom_dict_fear <- data.frame(word=FEEL$word, value=FEEL$fear)
```

Et donc reprodurie une même visualisation à partir de ce nouveau sous-lexique

```{r, warning=FALSE, results='hide'}
#{r, fig.width=6, fig.height=, dpi=10}
text_to_analyse_vector_fear <- get_sentiment(text_to_analyse, method = "custom", lexicon = custom_dict_fear)
simple_plot(text_to_analyse_vector_fear, title = paste(text_title, "Fear", sep=" - "))
#Saving the graph
png(paste(file_name_noExt,"_FEAR.png"), height = 900, width = 1600, res = 100)
simple_plot(text_to_analyse_vector_fear, title = paste(text_title, "Fear", sep=" - "))
dev.off()
```

Extraire les mots utilisés pour l'analyse

```{r}
positive_words=character()
positive_words_custom=character()
for(i in 1:length(text_to_analyse_tokenised)){
  # identify positive words
  if(get_sentiment(names(text_to_analyse_tokenised[i]), method = "custom", lexicon = custom_dict_fear) > 0){
    positive_words = c(positive_words, names(text_to_analyse_tokenised[i]))
    positive_words_custom = c(positive_words_custom, (text_to_analyse_tokenised[i]*get_sentiment(names(text_to_analyse_tokenised[i]), method = "custom", lexicon = custom_dict_fear)))
  }
}
head(sort(positive_words_custom, decreasing = T), n=50)

```
`

### References
Un grand merci à Simone Rebora (Università di Verona/DHLab Basel) pour son aide.