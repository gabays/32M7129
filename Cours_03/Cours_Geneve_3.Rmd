---
title: "3. Mes premiers pas de TAListe: _topic modeling_, _LDA_ _et alii_"
author: "Simon Gabay"
date: "12/10/2021"
output:
  html_document:
    highlight: pygments
    toc: true
    toc_float:
      toc_collapsed: true
    theme: united
---

<a style="float:right; width: 20%;" rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Licence Creative Commons" src="https://i.creativecommons.org/l/by/4.0/88x31.png"/></a>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Pr√©paratifs

J'installe ma session de travail

```{r}
setwd("~/Documents/github/UNIGE/32M7129/Cours_03")
monDossier="~/Documents/github/UNIGE/32M7129/Cours_03"
#je charge les donn√©es que l'enseignant a pr√©par√© pour √©viter les probl√®mes
#load("Cours_Geneve_3.RData")
```

Premi√®re chose √† faire: importer le corpus qui se trouve dans le dossier `cours 3`. Comme il s'agit d'un csv, nous utilisons la fonction `read.csv()` Le corpus que nous importons est une collection de blocs d'environ 1000 mots lemmatis√©s.
(notez la pr√©sence de lignes commen√ßant par un di√®se. Il s'agit d'un commentaire: quand il est utilis√©, la ligne n'est pas interpr√©t√©e par R)

```{r}
theatre = "moliere_racine.tsv"
# le param√®tre `header` permet de signaler que la premi√®re ligne contient le nom des colonnes
# le param√®tre `sep` permet d'indiquer comment sont marqu√©es les colonnes. La regex `\t` indique que nous utilisons des tabulations (notre fichier est donc en fait un `tsv` et non un vrai `csv`).
# le param√®tre fileEncoding permet d'avoir des charact√®res encod√©s en UTF8 (si vous avez windows, sans cette option le r√©sultat de l'import peut √™tre probl√©matique)
theatre <- read.csv(theatre, header=TRUE, sep = "\t", quote = '', fill = TRUE, fileEncoding="UTF-8")
```

Je peux jeter un coup d'≈ìil aux donn√©es brutes (on ne m'affiche que les premi√®re entr√©es de chaque colonne par commodit√©)

```{r}
str(theatre)
```

Je peux aussi les regarder dans un tableau directement dans RStudio. On remarque que les colonnes ont des noms: "auteur", "titre"‚Ä¶

```{r}
View(theatre)
```

Je peux s√©lectionner juste une colonne (ici "auteur"). Afin de ne pas tout afficher j'utilise la fonction `head()` pour ne montrer que les premi√®res entr√©es:

```{r}
head(theatre$auteur)
# Je peux augmenter le nombre de r√©sultat affich√© en indiquant le chiffre souhait√© de la mani√®re suivante:
#head(theatre$auteur,10)
#Pour les derni√®res entr√©es, il existe une fonction `tail`
#tail(theatre$auteur)
```

Toutes les colonnes sont des m√©tadonn√©es, sauf `theatre$texteLemmat` qui contient des "morceaux" de pi√®ces de 1000 mots afin de simplifier le travail (nous allons y revenir). Il va falloir transformer le contenu de cette colonne en matrice terme-document (_Document Term Matrix_), c'est-√†-dire cr√©er un tableau avec une colonne pour chaque mot de mon corpus, et un rang par texte de mon corpus.

|        | mot1 | mot2 | mot3 |
|--------|------|------|------|
| Texte1 |  1   |  12  |  9   |
| Texte2 |  1   | 154  |  4   |

C'est le principe d'une approche _bag of words_, c'est √† dire par "sac de mots": les mots ne sont pas pris dans leur contexte, uniquement par leur fr√©quence. Cela peut para√Ætre un peu rustre, mais c'est tr√®s efficace.

```{r}
#Je charge deux nouvelles librairies pour le _text mining_ qui me permettent de cr√©er ma matrice
if(!require("tm")){
  install.packages("tm")
  library("tm")
}
if(!require("tidytext")){
  install.packages("tidytext")
  library("tidytext")
}
# Je transforme mes textes en corpus avec la fonction `corpus()`, un objet de classe `corpus` manipulable dans `R` contenant des donn√©es et des m√©tadonn√©es
#La fonction `VectorSource` transforme chaque document en vecteur
corpus <- Corpus(VectorSource(theatre$texteLemmat), readerControl = list(language = "fr"))
# J'affiche les informations √† propos de ce corpus
corpus
```

Je peux d√©sormais "utiliser" cet objet:

```{r}
#je compte le nombre de colonne dans ma matrice
ncol(as.matrix(DocumentTermMatrix(corpus)))
#J'affiche le premier vecteur de mon objet `corpus`:
corpus[[1]][[1]]
```

# 2. Je nettoie mon corpus

Il est absolument fondamental de nettoyer mon corpus de travail. En effet: _pas_ et _Pas_ ne sont pas les m√™mes cha√Ænes de caract√®res (il y a une majuscule dans le second), et peut-√™tre m√™me pas les m√™mes mots (adverbe ou substantif?). Je dois donc au moins retirer les majuscules (avec la fonction `tolower()`), ou m√™me lemmatiser (de pr√©f√©rence avec un outil sp√©cifique, qui n'existe pas dans `R`).Pour rappel,nous fournissons ici le texte pr√©alablement lemmatis√© pour simplifier le travail.

## 2.1 Les _stopwords_

Comme notre objectif est d'avoir une approche th√©matique et conserver des mots potentiellement porteurs de sens: il faut donc retirer tous les mots les plus fr√©quents qui n'apportent, comme les les pronoms, les pronoms adverbiaux, les pr√©positions‚Ä¶  Ces mots sont appel√©s des _stopwords_ et une liste est fournie dans la fonction `stopwords()`

```{r}
stopwords("french")
```

Il existe des listes alternatives en ligne, plus compl√®tes:

```{r}
#Donner un nom au fichier que je t√©l√©charge
mesStops="stopwords-fr.csv"
#indiquer l'URL o√π se trouve le document √† t√©l√©charger
stopword_enLigne = "https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt"
#t√©l√©charger le fichier et l'enregistrer sous le nom que je viens de lui donner
download.file(stopword_enLigne,mesStops)
#Comme c'est un tableur, je le lis avec la fonction ad√©quat 
stopword_enLigne = read.csv(stopword_enLigne, header=FALSE, stringsAsFactors=FALSE)[,]
#je jette un coup d'≈ìil aux 10 premiers
head(stopword_enLigne,10)
```

Je vais utiliser mes listes de _stopwords_ l'une apr√®s l'autre pour nettoyer mon corpus. Pour cela j'utilise la fonction `tm_map()` qui permet de modifier les corpora. Dans ce cas pr√©cise j'utilise `removeWords` avec chacune des deux listes.

```{r, error=FALSE}
corpus_clean <- tm_map(corpus, removeWords, stopwords("french"))
corpus_clean <- tm_map(corpus, removeWords, stopword_enLigne)
#Je jette un coup d'≈ìil √† la sixi√®me entr√©e pour contr√¥ler que tout est en ordre
inspect(corpus_clean[6])
```

Malheureusement cette commande `tm_map()` fonctionne mal, et il est pr√©f√©rable de nettoyer le texte "√† l'ancienne", en cr√©ant sa propore fonction.

```{r, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
#Je recharge mon corpus
corpus_clean <- tm_map(corpus_clean, PlainTextDocument)
#je cr√©e une fonction a deux param√®tres: le corpus d'entr√©e et la liste des stopwords.
removeStopWords <- function(corpus_a_nettoyer, stopwords_a_retirer){
  # je fais une boucle pour retirer chaque mot de `stopwords_a_retirer`
  for (word in stopwords_a_retirer){
    #J'utilise une fonction anonyme (_snonymous function_) √† un param√®tre qui utilise la fonction `gsub` qui remplace le mot de `stopwords_a_retirer` par rien.
    removeWord <- function(x) gsub(paste("(^|\\s)(",word,") ", sep="")," ",x)
    #on retire le mot
    corpus_a_nettoyer <- tm_map(corpus_a_nettoyer, removeWord)
  }
  #Je renvoie le r√©sultat
  return(corpus_a_nettoyer)
}

#Je passe mon `corpus_clean` comme `corpus_a_nettoyer` et mes `stopword_enLigne` comme `stopwords_a_retirer`.
corpus_clean <- removeStopWords(corpus_clean, stopword_enLigne)
```

S'il reste des mots qui ne me plaisent pas, je peux continuer de les retirer en les mettant dans un vecteur

```{r}
MesStopWords <- c( "√†_le", "de_le", "-√™tre", "faire", "falloir", "savoir", "pouvoir", "devoir", "devoir", "voir", "vouloir")
corpus_cleaner <- tm_map(corpus_clean, removeWords, MesStopWords)
inspect(corpus_cleaner[6])
```

Je fais de nouveau une matrice "terme/document" (DTM, _Document-term matrix_). On se rappelle qu'il s'agit de cr√©er une matrice (un tableau) avec une colonne pour chaque mot de mon corpus, et un rang par texte de mon corpus.

|        | mot1 | mot2 | mot3 |
|--------|------|------|------|
| Texte1 |  1   |  12  |  9   |
| Texte2 |  1   | 154  |  4   |


```{r}
dtm <- DocumentTermMatrix(corpus_cleaner)
rownames(dtm) <- theatre$genre
```


## 2.2 Les mots peu fr√©quents

Je peux d√©sormais observer la fr√©quence des mots: je retrouve la loi de Zipf dans la distribution de mes donn√©es

```{r}
freq <- as.data.frame(colSums(as.matrix(dtm)))
colnames(freq) <- c("frequence")
#Comme je vais dessiner un graph, j'ai besoin d'une nouvelle librairie: `ggplot2`
if (!require("ggplot2")){
  install.packages("ggplot2")
  library("ggplot2")
}
#Je dessine mon graph
ggplot(freq, aes(x=frequence)) + geom_density()
```

Je peux compter les mots avec des fr√©quences faibles, par exemple avec moins de 100 occurrences

```{r}
#Je retire tous les mots qui apparaissent entre 0 et 400 fois (on peut remplacer 400 par 100, ou m√™me 10 si le corpus est trop gros)
motsPeuFrequents <- findFreqTerms(dtm, 0, 400)
#Si vous √™ts sur windows, d√©commentez la ligne suivante
#Encoding(motsPeuFrequents)<-"latin-1"
length(motsPeuFrequents)
head(motsPeuFrequents,50)
```

Je peux aussi compter et afficher les mots les plus fr√©quents, par exemple avec plus de 400 occurrences

```{r}
motsTresFrequents <- findFreqTerms(dtm, 401, Inf)
#Si vous √™ts sur windows, d√©commentez la ligne suivante
#Encoding(motsTresFrequents)<-"latin-1"
length(motsTresFrequents)
head(motsTresFrequents,50)
```

Je fais un tr√®s grand m√©nage, avec une fonction que je cr√©e pour retirer les mots les moins fr√©quents:

```{r}
#Je cr√©e une fonction `grandMenage`
grandMenage <- function(corpus_a_nettoyer, mots_peu_importants){
  #Afin de simplifier le travail (de mon ordinateur), je vais rassembler les mots √† retirer en groupe 500 tokens, que je vais traiter s√©par√©ment.
    chunk <- 500
    #Je compte le nombre de mots √† retirer
    n <- length(mots_peu_importants)
    #Je compte les groupes de 500 (ici 17.05), j'arrondis au plus petit entier sup√©rieur (ici 18) 
    r <- rep(1:ceiling(n/chunk),each=chunk)[1:n]
    #Je constitue mes lots sur la base du d√©compte pr√©c√©demment mentionn√©
    d <- split(mots_peu_importants,r)
    #Je fais une boucle: pour retirer les mots du corpus, morceau par morceau
    for (i in 1:length(d)) {
        corpus_a_nettoyer <- tm_map(corpus_a_nettoyer, removeWords, c(paste(d[[i]])))
    }
    #Je renvoie un r√©sultat
    return(corpus_a_nettoyer)
}
# J'utilise ma fonction avec `corpus_clean` comme ` corpus_a_nettoyer` et `motsPeuFrequents` comme `mots_peu_importants`
corpus_cleanSuperClean <- grandMenage(corpus_cleaner, motsPeuFrequents)
```

Je red√©finis ma matrice √† partir de mon nouveau corpus

```{r} 
dtm <- DocumentTermMatrix(corpus_cleanSuperClean)
rownames(dtm) <- theatre$genre
freq <- as.data.frame(colSums(as.matrix(dtm)))
colnames(freq) <- c("frequence")
#Je fais un petit graph
ggplot(freq, aes(x=frequence)) + geom_density()
```

Je nettoye un peu ma DTM pour √©liminer les rangs vides

```{r}
rowTotals <- apply(dtm , 1, sum)      #Find the sum of words in each Document
dtm_clean   <- dtm[rowTotals> 0, ]    #remove all docs without words
```

# 3. _Topic modeling_

**Remarque pr√©liminaire**: le _topic modeling_ requiert des (tr√®s) grands corpus, si possible en centaines de documents. Pas de panique cependant: une mani√®re de les obtenir est de diviser chaque textes en plusieurs documents qui forment une unit√© s√©mantique. Par exemple le chapitre, la sc√®ne, le paragraphe, ou bien (comme c'est le cas pour notre exercice) de 1000 mots.

## 3.1 Explication th√©orique

Un th√®me (_topic_) est un _clusters_ de mots, _i.e._ une r√©currence de co-occurrence.

![100% center](images/co-occurrence.png)

Source: [Wikisource](https://commons.wikimedia.org/wiki/File:Khcoder_net_e.png)

Le principe du _topic modeling_ est proche de celui de surligner un texte avec plusieurs couleurs: une pour chaque sujet, th√®me ou _topic_.

![100% center](images/souligner.png)

Une telle image soul√®ve deux questions sur lesquelles nous reviendront plus tard:
* un article peut-il contenir plusieurs sujets?
* un mot peut-il n'appartenir qu'√† un seul sujet?

Afin de reconna√Ætre ces sujets, on va recourir √† une allocation de Dirichlet latente ( _Latent Dirichlet allocation_, LDA).
* C'est une approche non supervis√©e, c'est-√†-dire qu'elle ne n√©cessite pas d'annotation pr√©alable de donn√©es.
* Il nous faut d√©finir √† l'avance un nombre de sujets/th√®mes (_infra_ la variable `k`)

Le _LDA_  est mod√®le g√©n√©ratif probabiliste permettant d‚Äôexpliquer des ensembles d‚Äôobservations, par le moyen de groupes non observ√©s, eux-m√™mes d√©finis par des similarit√©s de donn√©es.

![150% center](images/LDA_formula.png)

Source: [wikipedia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)

Dans ce graph:
* _M_ est le nombre de documents (corpus)
* _N_ est le nombre de mots (document)
* _W_ est un mot observ√©

La partie _latente_ (cach√©e):
* _Z_ est un _topic_ attribu√© √† un _w_
* _Œ∏_ est le m√©lange des _topics_ √† l'√©chelle du document

Deux param√®tres pour la distribution
* _Œ±_ est la distribution par document. Si sa valeur est √©lev√©e, le document tend √† contenir plusieurs _topics_, si la valeur est faible le nombre de _topics_ est limit√© 
* _Œ≤_ est la distribution par _topic_. Si sa valeur est √©lev√©e, un m√™me mot se retrouve dans plusieurs _topics_ (qui se ressemblent donc), si la valeur est faible les similarit√©s entre les _topics_ est faible 

![150% center](images/LDA_dessin.png)

Source: [wikipedia](https://commons.wikimedia.org/wiki/File:Latent_Dirichlet_allocation.svg)

## 3.2 Une LDA

Le mod√®le va classer al√©atoirement tous les mots en _n_ sujets, et tenter d'affiner cette r√©partition de mani√®re it√©rative en observant les contextes:

```{r}
#J'installe une nouvelle librairie pour le _topic modeling_
if(!require("topicmodels")){
  install.packages("topicmodels")
  library("topicmodels")
}
#Je vais partir sur une classification en deux _topics_
k = 2
lda_2 <- LDA(dtm_clean, k= k, control = list(seed = 1234))
##Je tente avec trois, pour voir‚Ä¶
lda_3 <- LDA(dtm_clean, k= k+1, control = list(alpha = 0.1))
```

Le r√©sultat produit est une matrice avec pour chaque mot la probabilit√© qu'il appartienne √† un des diff√©rents _topics_. On donne un score _Œ≤_, qui est celui pr√©sent√© infra.

```{r}
topics <- tidy(lda_2, matrix = "beta")
topics
```

## 3.3 Les param√®tres de Gibbs

Les param√®tres de Gibbs permettent une sophistication du syst√®me pr√©c√©dent. C'est une probabilit√© conditionnelle qui s'appuie, pour calculer le _Œ≤_ d'un mot, sur le _Œ≤_ des mots voisins. Pour ce faire nous devons d√©terminer:
1. √Ä quel point un document aime un _topic_
2. √Ä quel point un _topic_ aime un mot

Un document:

| Voiture | Autoroute | Musique | V√©lo | Vacances |
|---------|-----------|---------|------|----------|
|    1    |    ??     |    2    |  1   |     3    |

Sachant que le d√©compte est le suivant

|           | topic 1 | topic 2 | topic 3 |
|-----------|---------|---------|---------|
| Voiture   |   34    |   49    |    75   |
| Autoroute |   150   |   50    |    70   |
| Musique   |   34    |    4    |   170   |
| V√©lo      |   543   |    2    |   150   |
| Vacances  |   23    |   70    |   563   |

Le _topic_ 1 est le plus repr√©sent√© dans le document, et _Autoroute_ est d√©j√† surrepr√©sent√© dans le d√©compte, donc on _update_ le tout

| Voiture | Autoroute | Musique | V√©lo | Vacances |
|---------|-----------|---------|------|----------|
|    1    |   **1**   |    2    |  1   |     3    |

|           | topic 1 | topic 2 | topic 3 |
|-----------|---------|---------|---------|
| Voiture   |   34    |   49    |    75   |
| Autoroute | **151** |   50    |    70   |
| Musique   |   34    |    4    |   170   |
| V√©lo      |   543   |    2    |   150   |
| Vacances  |   23    |   70    |   563   |

Nous devons d'abord d√©terminer le nombre optimal de _topics_

```{r}

#J'installe une nouvelle librairie pour d√©terminer le nombre de topics
if(!require("ldatuning")){
  install.packages("ldatuning")
  library("ldatuning")
}
#J'ex√©cute le calcul
topicsNumber <- FindTopicsNumber(
  #La DTM que j'utilise
  dtm_clean,
  #Le nombre de possibilit√©s que je teste
  topics = seq(from = 2, to = 20, by = 1),
  #Les m√©triques utilis√©es
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)
#J'affiche le r√©sultat
FindTopicsNumber_plot(topicsNumber)
```


```{r}
## Set parameters for Gibbs sampling
#Le mod√®le va tourner 2000 fois avant de commencer √† enregistrer les r√©sultats
burnin <- 2000
#Apr√®s cela il va encore tourner 2000 fois
iter <- 2000
# Il ne va enregistrer le r√©sultat que toutes les 500 it√©rations
thin <- 500
#seed et nstart pour la reproductibilit√©
SEED=c(1, 2, 3, 4, 5)
seed <-SEED
nstart <- 5
#Seul meilleur mod√®le est utilis√©
best <- TRUE
#7 topics
lda_gibbs_7 <- LDA(dtm_clean, 7, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
#19 topics
lda_gibbs_19 <- LDA(dtm_clean, 19, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
```

Je peux d√©sormais voir les premiers r√©sultats pour chacun des mod√®les. Il s'agit de de mots dont la fr√©quence d'utilisation est corr√©l√©e

```{r}
"LDA 2"
termsTopic <- as.data.frame(terms(lda_2,10))
head(termsTopic,11)
"LDA 3"
termsTopic <- as.data.frame(terms(lda_3,10))
head(termsTopic,11)
"LDA GIBBS 7"
termsTopic <- as.data.frame(terms(lda_gibbs_7,10))
head(termsTopic,11)
"LDA GIBBS 19"
termsTopic <- as.data.frame(terms(lda_gibbs_19,10))
head(termsTopic,11)
```

Nous allons utiliser `lda_gibbs_2` et construire une matrice avec les _Œ≤_ des tokens (pour les …£, et donc des probabilit√©s par document, on aurait mis `matrix = "gamma"`). Chaque token est r√©p√©t√© deux fois, avec une probabilit√© pour chaque _topic_:

```{r}
topics <- tidy(lda_gibbs_7, matrix = "beta")
topics
```

# 4. Visualisation

```{r}
#Je vais encore solliciter une nouvelle librairie
if (!require("dplyr")){
   install.packages("dplyr")
  library("dplyr")
}

#Je r√©cup√®re mes mots
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup()  %>%
  arrange(topic, -beta)
#Je fais un graph
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) +
                                                  facet_wrap(~ topic, scales = "free") +
                                                  coord_flip() +
                                                  scale_x_reordered()
```

Je vais d√©sormais associer chaque mot √† l'un des 5 genres possibles, pour d√©terminer auquel mes tokens sont rattach√©s, et d√©couvrir (potentiellement quel genre se cacher derri√®re quel _topic_

```{r}
if (!require("reshape2")){
  install.packages("reshape2")
  library("reshape2")
}
df <- melt(as.matrix(dtm_clean))
df <- df[df$Terms %in% findFreqTerms(dtm_clean, lowfreq = 800), ]
ggplot(df, aes(as.factor(Docs), Terms, fill=log(value))) +
                                             geom_tile() +
                                             xlab("Genres") +
                                             scale_fill_continuous(low="#FEE6CE", high="#E6550D") +
                                             theme(axis.text.x = element_text(angle=90, hjust=1))
```

```{r, fig.width=12, fig.height=12}
tt <- posterior(lda_gibbs_7)$terms
melted = melt(tt[,findFreqTerms(dtm_clean, 1000,10000)])

colnames(melted) <- c("Topics", "Terms", "value")
melted$Topics <- as.factor(melted$Topics)
ggplot(data = melted, aes(x=Topics, y=Terms, fill=value)) + 
                                              geom_tile() +
                                              theme(text = element_text(size=35))
```

```{r, fig.width=12, fig.height=12}
tt <- posterior(lda_gibbs_19)$terms
melted = melt(tt[,findFreqTerms(dtm_clean, 1000,10000)])

colnames(melted) <- c("Topics", "Terms", "value")
melted$Topics <- as.factor(melted$Topics)
ggplot(data = melted, aes(x=Topics, y=Terms, fill=value)) + 
                                              geom_tile() +
                                              theme(text = element_text(size=35))
```

On peut aussi observer le score gamma, c'est-√†-dire la probabilt√© qu'un document contienne un sujet:

```{r}
DocumentTopicProbabilities <- as.data.frame(lda_gibbs_19@gamma)
rownames(DocumentTopicProbabilities) <- rownames(corpus_cleanSuperClean)
head(DocumentTopicProbabilities)
```

Nous allons d√©sormais faire des _word clouds_. Pour cela appelons (installons?) les libraries suivantes:

```{r}
if (!require("wordcloud")){
   install.packages("wordcloud")
  library("wordcloud")
}
if (!require("RColorBrewer")){
   install.packages("RColorBrewer")
  library("RColorBrewer")
}
if (!require("wordcloud2")){
   install.packages("wordcloud2")
  library("wordcloud2")
}
```

je r√©cup√®re les mots et je les associe √† leur ùõÉ

```{r, fig.width=20, fig.height=20}
tm <- posterior(lda_gibbs_7)$terms
data = data.frame(colnames(tm))
head(data)
```
Je produis une visualisation par _topic_

```{r, fig.width=30, fig.height=20}
for(topic in seq(k)){
    data$topic <-tm[topic,]
    #text(x=0.5, y=1, paste("V",topic, sep=""),cex=0.6)
    wordcloud(
      words = data$colnames.tm.,
      freq = data$topic,
      #sous ce seuil, les mots ne seront pas affich√©s
      min.freq=0.0002,
      #nombre maximum de mots √† afficher
      max.words=30,
      #Si faux, en ordre croissant
      random.order=FALSE,
      #% de mots √† 90¬∞
      rot.per=.35,
      #taille du graph
      scale=c(10,10),
      #couleurs
      colors = brewer.pal(5, "Dark2")
      # il est possible de rentrer directement les couleurs qui nous int√©ressent
      #c("red", "blue", "yellow", "chartreuse", "cornflowerblue", "darkorange")
    )
}
```

Finissons avec un peu de mauvais go√ªt, gr√¢ce au package `wordcloud2`

```{r, fig.width=20, fig.height=20}
wordcloud2(data = data,
          size=0.4,
          color= "random-light",
          backgroundColor = "pink",
          shape = 'star',
          rotateRatio=1
    )
```

# Rermerciements
Les donn√©es d'entra√Ænement ont √©t√© cr√©√©es par JB Camps (ENC).
Des morceaux de ce script (notamment pour le nettoyage des donn√©es) proviennent d'un cours de Mattia Egloff (UniL).